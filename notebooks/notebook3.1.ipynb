{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "neural ir.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9egyhuvU3_GI"
      },
      "source": [
        "# PyTerrier ECIR 2021 Tutorial Notebook - Part 3.1 - OpenNIR and monoT5\n",
        "\n",
        "This is one of a series of Colab notebooks created for This notebook provides experiences to attendees for creating indexing pipelines in PyTerrier. All experiments are conducted using the CORD19 corpus and the TREC Covid test collection.\n",
        "\n",
        "This notebook aims to demonstrate use of the neural re-rankers using the [OpenNIR](https://opennir.net/) and [monoT5](https://github.com/terrierteam/pyterrier_t5) plugins.\n",
        "\n",
        "In this notebook, you will experience:\n",
        "\n",
        " - re-ranking documents using neural models like KNRM, Vanilla BERT, EPIC, and monoT5.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dl0-Gs6e5I7n"
      },
      "source": [
        "## Setup\n",
        "\n",
        "In the following, we will set up the libraries required to execute the notebook.\n",
        "\n",
        "### Pyterrier installation\n",
        "\n",
        "The following cell installs the latest release of the [PyTerrier](https://github.com/terrier-org/pyterrier) package."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zSgDzjKxqfq5"
      },
      "source": [
        "!pip install --upgrade python-terrier"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mV0C6jJvqhMR"
      },
      "source": [
        "### Pyterrier plugins installation \n",
        "\n",
        "We install the official version of the [OpenNIR](https://opennir.net/) and [monoT5](https://github.com/terrierteam/pyterrier_t5) plugins. PyTerrier plugins. You can safely ignore the package versioning errors."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AkIR_PXdet7R"
      },
      "source": [
        "!pip install --upgrade git+https://github.com/Georgetown-IR-Lab/OpenNIR\n",
        "!pip install --upgrade git+https://github.com/terrierteam/pyterrier_t5"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A-nQrpNP5pN7"
      },
      "source": [
        "## Preliminary steps\n",
        "\n",
        "### [PyTerrier](https://github.com/terrier-org/pyterrier) initialization \n",
        "\n",
        "Lets get [PyTerrier](https://github.com/terrier-org/pyterrier) started. This will download the latest version of the [Terrier](http://terrier.org/) IR platform. We also import the [OpenNIR](https://opennir.net/) pyterrier bindings."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6FegcyWr5lja"
      },
      "source": [
        "import pyterrier as pt\n",
        "if not pt.started():\n",
        "    pt.init(tqdm='notebook')\n",
        "import onir_pt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hPVjr448rIPc"
      },
      "source": [
        "### [TREC-COVID19](https://ir.nist.gov/covidSubmit/) Dataset download\n",
        "\n",
        "The following cell downloads the [TREC-COVID19](https://ir.nist.gov/covidSubmit/) dataset that we will use in the remainder of this notebook."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IJMHFRfArd7O"
      },
      "source": [
        "dataset = pt.datasets.get_dataset('irds:cord19/trec-covid')\n",
        "topics = dataset.get_topics(variant='description')\n",
        "qrels = dataset.get_qrels()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OF3HIPhtrqOH"
      },
      "source": [
        "### Terrier inverted index download\n",
        "\n",
        "To save a few minutes, we use a pre-built Terrier inverted index for the TREC-COVID19 collection. Download time took a few seconds for us.\n",
        "\n",
        "The construction of the inverted index will take few minutes, and the code to use is the following:\n",
        "\n",
        "```python\n",
        "import os\n",
        "\n",
        "cord19 = pt.datasets.get_dataset('irds:cord19/trec-covid')\n",
        "pt_index_path = './terrier_cord19'\n",
        "\n",
        "if not os.path.exists(pt_index_path + \"/data.properties\"):\n",
        "    # create the index, using the IterDictIndexer indexer \n",
        "    indexer = pt.index.IterDictIndexer(pt_index_path)\n",
        "\n",
        "    # we give the dataset get_corpus_iter() directly to the indexer\n",
        "    # while specifying the fields to index and the metadata to record\n",
        "    index_ref = indexer.index(cord19.get_corpus_iter(), \n",
        "                              fields=('abstract',), \n",
        "                              meta=('docno',))\n",
        "\n",
        "else:\n",
        "    # if you already have the index, use it.\n",
        "    index_ref = pt.IndexRef.of(pt_index_path + \"/data.properties\")\n",
        "\n",
        "index = pt.IndexFactory.of(index_ref)\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6oCcP90yrlGi"
      },
      "source": [
        "import os\n",
        "\n",
        "if not os.path.exists(\"terrier_index.zip\"):\n",
        "  !wget http://www.dcs.gla.ac.uk/~craigm/ecir2021-tutorial/terrier_index.zip\n",
        "  !unzip -j terrier_index.zip -d terrier_index\n",
        "\n",
        "index_ref = pt.IndexRef.of(\"./terrier_index/data.properties\")\n",
        "index = pt.IndexFactory.of(index_ref)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rwDams5M7g6c"
      },
      "source": [
        "## Re-Rankers from scratch\n",
        "\n",
        "Let's start exploring a few neural re-ranking methods! We can build them from scratch using `onir_pt.reranker`.\n",
        "\n",
        "And OpenNIR reranking model consists of:\n",
        " - `ranker` (e.g., `drmm`, `knrm`, or `pacrr`). This defines the neural ranking architecture.\n",
        " - `vocab` (e.g., `wordvec_hash`, or `bert`). This defines how text is encoded by the model. This approach makes it easy to swap out different text representations."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F0O79K2K6fvn"
      },
      "source": [
        "knrm = onir_pt.reranker('knrm', 'wordvec_hash', text_field='abstract')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x1avVTpxDORN"
      },
      "source": [
        "Let's look at how well these models work at ranking!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4FWUIN577v1O"
      },
      "source": [
        "br = pt.BatchRetrieve(index) % 100\n",
        "pipeline = br >> pt.text.get_text(dataset, 'abstract') >> knrm\n",
        "pt.Experiment(\n",
        "    [br, pipeline],\n",
        "    topics,\n",
        "    qrels,\n",
        "    names=['DPH', 'DPH >> KNRM'],\n",
        "    eval_metrics=[\"map\", \"ndcg\", 'ndcg_cut.10', 'P.10', 'mrt']\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MhquobQypVNJ"
      },
      "source": [
        "This doesn't work very well because the model is not trained; it's using random weights to combine the scores from the similarity matrix."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WLv7quA43yAP"
      },
      "source": [
        "## Loading a trained re-ranker\n",
        "\n",
        "You can train re-ranking models in PyTerrier using the `fit` method. This takes a bit of time, so we'll download a model that's already been trained. If you'd like to train the model yourself, you can use:\n",
        "\n",
        "```python\n",
        "# transfer training signals from a medical sample of MS MARCO\n",
        "from sklearn.model_selection import train_test_split\n",
        "train_ds = pt.datasets.get_dataset('irds:msmarco-passage/train/medical')\n",
        "train_topics, valid_topics = train_test_split(train_ds.get_topics(), test_size=50, random_state=42) # split into training and validation sets\n",
        "\n",
        "# Index MS MARCO\n",
        "indexer = pt.index.IterDictIndexer('./terrier_msmarco-passage')\n",
        "tr_index_ref = indexer.index(train_ds.get_corpus_iter(), fields=('text',), meta=('docno',))\n",
        "\n",
        "pipeline = (pt.BatchRetrieve(tr_index_ref) % 100 # get top 100 results\n",
        "            >> pt.text.get_text(train_ds, 'text') # fetch the document text\n",
        "            >> pt.apply.generic(lambda df: df.rename(columns={'text': 'abstract'}) # rename columns\n",
        "            >> knrm) # apply neural re-ranker\n",
        "\n",
        "pipeline.fit(\n",
        "    train_topics,\n",
        "    train_ds.get_qrels(),\n",
        "    valid_topics,\n",
        "    train_ds.get_qrels())\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yk7FBOgvDa8V"
      },
      "source": [
        "del knrm # free up the memory before loading a new version of the ranker\n",
        "knrm = onir_pt.reranker.from_checkpoint('https://macavaney.us/knrm.medmarco.tar.gz', text_field='abstract', expected_md5=\"d70b1d4f899690dae51161537e69ed5a\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1BQQKv8lL0Ta"
      },
      "source": [
        "pipeline = br >> pt.text.get_text(dataset, 'abstract') >> knrm\n",
        "pt.Experiment(\n",
        "    [br, pipeline],\n",
        "    topics,\n",
        "    qrels,\n",
        "    names=['DPH', 'DPH >> KNRM'],\n",
        "    baseline=0,\n",
        "    eval_metrics=[\"map\", \"ndcg\", 'ndcg_cut.10', 'P.10', 'mrt']\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aI1_8O8rtXKK"
      },
      "source": [
        "That's a little better than before, but it still underperforms our first-stage ranking model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "79l1jn0pRQEY"
      },
      "source": [
        "## Vanilla BERT\n",
        "\n",
        "Contextualized language models, such as [BERT](https://arxiv.org/abs/1810.04805), are much more powerful neural models that have been shown to be effective for ranking.\n",
        "\n",
        "We'll try using a \"vanilla\" (or \"mono\") version of the BERT model. The BERT model is pre-trained for the task of language modeling and next sentence prediction."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-qlXPHqN3iO0"
      },
      "source": [
        "del knrm # clear out memory from KNRM\n",
        "vbert = onir_pt.reranker('vanilla_transformer', 'bert', text_field='abstract', vocab_config={'train': True})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "progrVwaunrn"
      },
      "source": [
        "Let's see how this model does on TREC COVID."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PkasovrjQjy0"
      },
      "source": [
        "pipeline = br % 100 >> pt.text.get_text(dataset, 'abstract') >> vbert\n",
        "pt.Experiment(\n",
        "    [br, pipeline],\n",
        "    topics,\n",
        "    qrels,\n",
        "    names=['DPH', 'DPH >> VBERT'],\n",
        "    baseline=0,\n",
        "    eval_metrics=[\"map\", \"ndcg\", 'ndcg_cut.10', 'P.10', 'mrt']\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wBrfNZ_1u_pD"
      },
      "source": [
        "As we see, although the model is pre-trained, it doesn't do very well at ranking on our benchmark. This is because it's not tuned for the task of relevance ranking.\n",
        "\n",
        "We can train the model for ranking (as shown above for KNRM) or we can download a trained model. Here, we use the [SLEDGE](https://arxiv.org/abs/2010.05987) model, which is a Vanilla BERT model trained on scientific text and tuned on medical queries."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VsXQKNyYSXOj"
      },
      "source": [
        "vbert = onir_pt.reranker.from_checkpoint('https://macavaney.us/scibert-medmarco.tar.gz', text_field='abstract', expected_md5=\"854966d0b61543ffffa44cea627ab63b\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dUH-daNJSoNy"
      },
      "source": [
        "pipeline = br % 100 >> pt.text.get_text(dataset, 'abstract') >> vbert\n",
        "pt.Experiment(\n",
        "    [br, pipeline],\n",
        "    topics,\n",
        "    qrels,\n",
        "    names=['DPH', 'DPH >> VBERT'],\n",
        "    baseline=0,\n",
        "    eval_metrics=[\"map\", \"ndcg\", 'ndcg_cut.10', 'P.10', 'mrt']\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OtAxDQHyv4ON"
      },
      "source": [
        "That's much better! We're able to improve upon the first stage ranker. But we can see that this is pretty slow to run."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xRcBOIBPRJre"
      },
      "source": [
        "## EPIC"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ef4O3PyAHMuS"
      },
      "source": [
        "Some models focus on query-time computational efficiency. The [EPIC](https://arxiv.org/abs/2004.14245) model builds light-weight document representations that are independent of the query. This means that they can be computed ahead of time. You can index the corpus yourself with the following code (but it takes a while):\n",
        "\n",
        "```python\n",
        "indexed_epic = onir_pt.indexed_epic.from_checkpoint('https://macavaney.us/epic.msmarco.tar.gz', index_path='./epic_cord19')\n",
        "indexed_epic.index(dataset.get_corpus_iter(), fields=('abstract',))\n",
        "```\n",
        "\n",
        "Instead, we'll download a copy of the EPIC-processed documents:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OwkE23yfGl65"
      },
      "source": [
        "import os\n",
        "if not os.path.exists('epic_cord19.zip'):\n",
        "  !wget http://www.dcs.gla.ac.uk/~craigm/ecir2021-tutorial/epic_cord19.zip\n",
        "  !unzip epic_cord19.zip\n",
        "indexed_epic = onir_pt.indexed_epic.from_checkpoint('https://macavaney.us/epic.msmarco.tar.gz', index_path='./epic_cord19')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y7LfbxJqw80B"
      },
      "source": [
        "We can now run this model over the results of a first-stage ranker. Note how we do not need to fetch the document text with `pt.text.get_text`, which further saves time."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tynDMGCYJCUv"
      },
      "source": [
        "br = pt.BatchRetrieve(index) % 30\n",
        "pipeline = (br >> indexed_epic.reranker())\n",
        "pt.Experiment(\n",
        "    [br, pipeline],\n",
        "    dataset.get_topics('description'),\n",
        "    dataset.get_qrels(),\n",
        "    names=['DPH', 'DPH >> EPIC (indexed)'],\n",
        "    eval_metrics=[\"map\", \"recip_rank\", \"ndcg\", \"ndcg_cut.10\", \"mrt\"]\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iCF-DglUPrB-"
      },
      "source": [
        "## Tuning re-ranking threshold\n",
        "\n",
        "[Prior work suggests](https://arxiv.org/pdf/1904.12683.pdf) that the re-ranking cutoff threshold is an important model hyperparameter. Let's see how this parameter affects EPIC."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "osnHkyGtNy1m"
      },
      "source": [
        "cutoffs = [10, 20, 30, 40, 50, 60, 70, 80, 90, 100]\n",
        "res = pt.Experiment(\n",
        "    [pt.BatchRetrieve(index) % cutoff >> indexed_epic.reranker() for cutoff in cutoffs],\n",
        "    dataset.get_topics('description'),\n",
        "    dataset.get_qrels(),\n",
        "    names=[f'c={cutoff}' for cutoff in cutoffs],\n",
        "    eval_metrics=[\"map\", \"recip_rank\", \"ndcg\", \"ndcg_cut.10\", \"mrt\"]\n",
        ")\n",
        "res"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yvq1gw8VO_rp"
      },
      "source": [
        "from matplotlib import pyplot as plt\n",
        "%matplotlib inline\n",
        "plt.plot(res['name'], res['ndcg_cut_10'], label='ndcg_cut_10')\n",
        "plt.plot(res['name'], res['recip_rank'], label='recip_rank')\n",
        "plt.ylabel('ndcg_cut_10 / recip_rank')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "plt.clf()\n",
        "plt.plot(res['name'], res['mrt'])\n",
        "plt.ylabel('mrt')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n8tu_yN4QRdh"
      },
      "source": [
        "It appears that the optimal re-ranking threshold for this collection is around 40-50. This also avoids excessive re-ranking time, which grows roughly linearly with larger thredhols. In pratice, this paramter should be tuned on a held-out validation set to avoid over-fitting."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GdUPsiYLX-L1"
      },
      "source": [
        "## monoT5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u1UoVYscxzR_"
      },
      "source": [
        "The [monoT5](https://arxiv.org/abs/2003.06713) model scores documents using a causal language model. Let's see how this approach works on TREC COVID.\n",
        "\n",
        "The `MonoT5ReRanker` class from `pyterrier_t5` automatically loads a version of the monoT5 ranker that is trained on the MS MARCO passage dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RuWuB44kQN2f"
      },
      "source": [
        "from pyterrier_t5 import MonoT5ReRanker\n",
        "monoT5 = MonoT5ReRanker(text_field='abstract')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5FI_KCicWDeB"
      },
      "source": [
        "br = pt.BatchRetrieve(index) % 30\n",
        "pipeline = (br >> pt.text.get_text(dataset, 'abstract') >> monoT5)\n",
        "pt.Experiment(\n",
        "    [br, pipeline],\n",
        "    dataset.get_topics('description'),\n",
        "    dataset.get_qrels(),\n",
        "    names=['DPH', 'DPH >> T5'],\n",
        "    eval_metrics=[\"map\", \"recip_rank\", \"ndcg\", \"ndcg_cut.10\", \"mrt\"]\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "#  That's all folks\n",
        "We have a separate notebook in Part 3 for ColBERT, as ColBERT needs a different version of Huggingface Trasformers. Refer to the [tutorial repostiory](https://github.com/terrier-org/ecir2021tutorial) for the other Part 3 notebook.\n",
        "\n",
        "If you arent coming back for Part 4 of the tutorial, please dont forget to complete our exit quiz: https://forms.office.com/r/2WbpLiQmWV"
      ],
      "cell_type": "markdown",
      "metadata": {}
    }
  ]
}